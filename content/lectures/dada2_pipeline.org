#+TITLE:     Amplicon Bioinformatic Analysis: DADA2
#+AUTHOR:    Josh Granek
#+DATE:      November 2, 2018


* Bioinformatic Goals
** Bioinformatic Analysis
*** Input: Raw FASTQ File(s)
   #+LATEX: \tiny
#+begin_src text
 M00698:36:000000000-AFBEL:1:1101:14738:1412 1:N:0:0
TTACGCTAACAGGCGGTAGCCTGGCAGGGTCAGGAAATCAATTAACTCATCGGAAGTGGTGATCTGTTCCATCAAGCGTGCGGCATCGTCA
+
ABBBABBBAFFFGGGGGGGGGGHGGHGGGCG2GF3FFGHHHHHHGGFGHEHHGGGEHHHHAGGHHGHHHFFDHFHHHGEGGGG@F@H?GHH
@M00698:36:000000000-AFBEL:1:1101:16483:1412 1:N:0:0
CTGCCAGTTGAACGACGGCGAGCAGTTATAAGCCAGCAGTTTGCCCGGATATTTCGCGTGGATAGCTTGTGCAAAGCGACGCGCCAGTTCC
+
AAABBFFFFFFFGGGGGGGGGGGGHHHHHHHHGHGHGHHHHHGHHHGGGGGHHHHGGGGGGGHHHGHHFFHHHHHGHGGGGGGGGGGHHHH
#+end_src
*** Output: Count Table
|            | Sample 1 | Sample 2 | …  | Sample N |
| /          | <>       | <>       | <> | <>       |
|------------+----------+----------+----+----------|
| Bacteria 1 |          |          |    |          |
|------------+----------+----------+----+----------|
| Bacteria 2 |          |          |    |          |
|------------+----------+----------+----+----------|
| …          |          |          |    |          |
|------------+----------+----------+----+----------|
| Bacteria N |          |          |    |          |
|------------+----------+----------+----+----------|

** Naive Approach: Assumptions
   - Library Prep is Perfect
   - Sequencing is Perfect
** Naive Approach: Counting
   #+ATTR_BEAMER: :overlay +(1)-
   1. Make an empty count table
   2. For each read in the FASTQ:
      1. If read sequence is already in count table, add 1 to that row
      2. Otherwise add a new row for the sequence and set its count to 1
** TODO Naive Approach: Counting Demo
*** A text section 						      :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    | Sequence | Count |
    | /        | <     |
    |----------+-------|
    |          |       |
    |          |       |
    |----------+-------|
    |          |       |
    |          |       |
    |----------+-------|
    |          |       |
    |          |       |
    |----------+-------|
    |          |       |
    |          |       |
    |----------+-------|
    |          |       |
    |          |       |
    |----------+-------|
    |          |       |
    |          |       |
    |----------+-------|

*** A screenshot 					    :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
1. CAGCT
2. TATAA
3. TATAA
4. TGCGC
5. CGGGC
6. TGCGC
7. TGCGC
8. CAGCT
9. CGGGC
10. TGCGC

** Generate Toy Reads						   :noexport:
```{r}
makeseq = function(seqlen=5){
  paste(sample(c("A","T","C","G"), seqlen, replace=TRUE),collapse = "")
}

# sample(rep(makeseq(), 5), rep(makeseq(), 2), rep(makeseq(), 1))
uniq_seqs = c(makeseq(), makeseq(), makeseq(), makeseq())
for (i in seq(10)){
  cat(paste0(i, "."), sample(uniq_seqs, 
                     prob = c(0.5, 0.2, 0.3, 0.3),
                     1, replace=TRUE), fill = TRUE)
}

```

```{r}
makeseq(15)
```

** Naive Assumptions
   - +Library Prep is Perfect+
   - +Sequencing is Perfect+
** Tools for Bioinformatic Analysis
*** A text section 						      :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.3
    :END:
   - "Clustering"
     - Mothur
     - UCLUST
     - UPARSE
   - "Denoising"
     - DADA2
     - UNOISE3
     - Deblur
*** A screenshot 					    :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.7
    :END:
    #+ATTR_LaTeX: :width 0.7\textwidth :float t :placement [H]
    file:figures/dada2_accuracy.jpg [fn::[[https://benjjneb.github.io/dada2/index.html][DADA2 Website]]]


* Get Data (pre-DADA2)
** Get Data: Sources
   - Sequence Read Archive (SRA)
   - MG-RAST (Metagenomic Rapid Annotations using Subsystems Technology)
   - Sequencing Facility
** Get Data: Tools
   - curl
   - wget
   - ncftp
   - rsync
   - sftp
   - SRA Toolkit
** Get Data: Result
   - FASTQ(s) (gzip'ed)
     - Undetermined_S0_L001_I1_001.fastq.gz
     - Undetermined_S0_L001_R1_001.fastq.gz
     - Undetermined_S0_L001_R2_001.fastq.gz
   - Map File*
     - mydata_map.txt
   - Checksum*
     - md5sum.txt
* Validate Data (pre-DADA2)
** Validate Data: Input
   - FASTQ(s) (gzip'ed)
     - Undetermined_S0_L001_I1_001.fastq.gz
     - Undetermined_S0_L001_R1_001.fastq.gz
     - Undetermined_S0_L001_R2_001.fastq.gz
   - Checksum*
     - md5sum.txt
   - Map File*
     - mydata_map.txt

** Validate Data: Output
# #+LATEX: \tiny
#+begin_src text
$ md5sum -c md5sum.txt
mydata_map.txt: OK
Undetermined_S0_L001_I1_001.fastq.gz: OK
Undetermined_S0_L001_R1_001.fastq.gz: OK
Undetermined_S0_L001_R2_001.fastq.gz: OK
#+end_src
** Validate Data: Tools
   - md5sum
* Assemble Metadata Table (pre-DADA2)
** Assemble Metadata Table: Why?
   Associate barcode with Sample
   - Label
   - Animal
   - Site
   - Phenotype
   - Treatment
   - Date
   - . . .
** Assemble Metadata Table: Input
   - Existing Map
   - Publication
   - Notes
** Assemble Metadata Table: Output
   Metadata Table (Mapping File)
   #+LATEX: \tiny

#+begin_src text
#SampleID	BarcodeSequence	LinkerPrimerSequence	Treatment	DOB	Description
PC.354	AGCACGAGCCTA	YATGCTGCCTCCCGTAGGAGT	Control	20061218	Control_mouse__I.D._354
PC.355	AACTCGTCGATG	YATGCTGCCTCCCGTAGGAGT	Control	20061218	Control_mouse__I.D._355
PC.356	ACAGACCACTCA	YATGCTGCCTCCCGTAGGAGT	Control	20061126	Control_mouse__I.D._356
PC.481	ACCAGCGACTAG	YATGCTGCCTCCCGTAGGAGT	Control	20070314	Control_mouse__I.D._481
PC.593	AGCAGCACTTGT	YATGCTGCCTCCCGTAGGAGT	Control	20071210	Control_mouse__I.D._593
PC.607	AACTGTGCGTAC	YATGCTGCCTCCCGTAGGAGT	Fast	20071112	Fasting_mouse__I.D._607
PC.634	ACAGAGTCGGCT	YATGCTGCCTCCCGTAGGAGT	Fast	20080116	Fasting_mouse__I.D._634
PC.635	ACCGCAGAGTCA	YATGCTGCCTCCCGTAGGAGT	Fast	20080116	Fasting_mouse__I.D._635
PC.636	ACGGTGAGTGTC	YATGCTGCCTCCCGTAGGAGT	Fast	20080116	Fasting_mouse__I.D._636
#+end_src

** Assemble Metadata Table: Output				   :noexport:
   Metadata Table (Mapping File)
   #+LATEX: \tiny

| #SampleID | BarcodeSequence | LinkerPrimerSequence  | Treatment |      DOB | Description             |
| PC.354    | AGCACGAGCCTA    | YATGCTGCCTCCCGTAGGAGT | Control   | 20061218 | Control_mouse__I.D._354 |
| PC.355    | AACTCGTCGATG    | YATGCTGCCTCCCGTAGGAGT | Control   | 20061218 | Control_mouse__I.D._355 |
| PC.356    | ACAGACCACTCA    | YATGCTGCCTCCCGTAGGAGT | Control   | 20061126 | Control_mouse__I.D._356 |
| PC.481    | ACCAGCGACTAG    | YATGCTGCCTCCCGTAGGAGT | Control   | 20070314 | Control_mouse__I.D._481 |
| PC.593    | AGCAGCACTTGT    | YATGCTGCCTCCCGTAGGAGT | Control   | 20071210 | Control_mouse__I.D._593 |
| PC.607    | AACTGTGCGTAC    | YATGCTGCCTCCCGTAGGAGT | Fast      | 20071112 | Fasting_mouse__I.D._607 |
| PC.634    | ACAGAGTCGGCT    | YATGCTGCCTCCCGTAGGAGT | Fast      | 20080116 | Fasting_mouse__I.D._634 |
| PC.635    | ACCGCAGAGTCA    | YATGCTGCCTCCCGTAGGAGT | Fast      | 20080116 | Fasting_mouse__I.D._635 |
| PC.636    | ACGGTGAGTGTC    | YATGCTGCCTCCCGTAGGAGT | Fast      | 20080116 | Fasting_mouse__I.D._636 |


** Assemble Metadata Table: Tools
   - Excel
   - Text Editor
   - Script
* Demultiplex (pre-DADA2)
** Demultiplex: Why?
   Split FASTQ File(s) by sample [fn:: Some data comes demultiplexed]
   # so reads for each sample are in their own FASTQ
** Demultiplex: Input
   - Sequence FASTQ(s)
     - Undetermined_S0_L001_I1_001.fastq.gz
     - Undetermined_S0_L001_R1_001.fastq.gz
   - Barcode FASTQ or Trimmed Versions [fn:: Some facilities incorporate barcodes in the sequence FASTQ, these will need to be extracted]
     - Undetermined_S0_L001_R2_001.fastq.gz 
   - Map File
     - mydata_map.txt 
** Demultiplex: Output
   Demultiplexed FASTQs
   - sampleA_R1.fastq.gz
   - sampleB_R1.fastq.gz
   - sampleC_R1.fastq.gz
   - . . .
   - sampleA_R2.fastq.gz
   - sampleB_R2.fastq.gz
   - sampleC_R2.fastq.gz
   - . . .
** Demultiplex: Tools
   - split_libraries_fastq.py + split_sequence_file_on_sample_ids.py
   - fastq_multx
* Adapter Trimming (pre-DADA2)
** Adapter Trimming: Why?
   Remove adapter contamination
   - Necessary for amplicons with large variation in length (e.g. ITS)
   - 
*** TODO figure?						   :noexport:
** Adapter Trimming: Input
*** Adapter Sequence
    my_adapter.fasta
*** Demultiplexed FASTQs
   - sampleA_R1.fastq.gz
   - sampleB_R1.fastq.gz
   - sampleC_R1.fastq.gz
   - . . .
   - sampleA_R2.fastq.gz
   - sampleB_R2.fastq.gz
   - sampleC_R2.fastq.gz
   - . . .
** Adapter Trimming: Output 
   Trimmed FASTQs
   - sampleA_R1.trim.fastq.gz
   - sampleB_R1.trim.fastq.gz
   - sampleC_R1.trim.fastq.gz
   - . . .
   - sampleA_R2.trim.fastq.gz
   - sampleB_R2.trim.fastq.gz
   - sampleC_R2.trim.fastq.gz
   - . . .
*** Synchronized Trimming
    Depending on settings, some reads may be thrown out during trimming.  It is essential that if a read is thrown out, its paired read is thrown out too.  Most trimming software will do this for you if you input R1 and R2 files when you run.

** Adapter Trimming: Tools
   - fastq_mcf
   - Trimmomatic
   - cutadapt
   - seqtk
   - etc

* Filter and Trim
** Filter and Trim: Why?
   - Remove low quality parts of reads
   - Remove reads that are low quality overall
** R1 Read Quality
    #+ATTR_LaTeX: :height 0.7\textheight :float t :placement [H]
    file:figures/see-quality-F-1.png [fn::[[https://benjjneb.github.io/dada2/tutorial_1_6.html][DADA2 Tutorial]]]

    # green is the mean
    # orange is the median
    # dashed orange lines are the 25th and 75th quantiles.

** R2 Read Quality
    #+ATTR_LaTeX: :height 0.7\textheight :float t :placement [H]
    file:figures/see-quality-R-1.png [fn::[[https://benjjneb.github.io/dada2/tutorial_1_6.html][DADA2 Tutorial]]]

** Filter and Trim: Input 
   Trimmed FASTQs (or Demultiplexed)
   - sampleA_R1.trim.fastq.gz
   - sampleB_R1.trim.fastq.gz
   - sampleC_R1.trim.fastq.gz
   - . . .
   - sampleA_R2.trim.fastq.gz
   - sampleB_R2.trim.fastq.gz
   - sampleC_R2.trim.fastq.gz
   - . . .
** Filter and Trim: Output
   Trimmed and filtered FASTQs
** Filter and Trim: Tools
   dada2::filterAndTrim()
** Filter and Trim: Parameters
   #+ATTR_BEAMER: :overlay +(1)-
   - truncQ: Truncate reads at the first instance of a quality score less than or equal to truncQ.
   - truncLen: Truncate reads after truncLen bases. *Don't use for ITS*
   - trimLeft: The number of nucleotides to remove from the start of each read.
   - minQ: After truncation, reads contain a quality score less than minQ will be discarded.
   - maxEE: After truncation, reads with higher than maxEE "expected errors" will be discarded. ~EE = sum(10^(-Q/10))~
   - rm.phix: Discard reads that match against the phiX genome
** Filter and Trim: Notes
   Paired-End Reads need to be run simultaneously to keep them in sync
* Learn Error Rates
** Learn Error Rates: Why?
   Build an error model from data
   | Phred | A:A | A:T | A:C | A:G | C:A | ... | G:G |
   |     / | <>  | <>  | <>  | <>  | <>  | <>  | <>  |
   |-------+-----+-----+-----+-----+-----+-----+-----|
   |     1 |     |     |     |     |     |     |     |
   |     2 |     |     |     |     |     |     |     |
   |     3 |     |     |     |     |     |     |     |
   |   ... |     |     |     |     |     |     |     |
   |    40 |     |     |     |     |     |     |     |

** Learn Error Rates: Input 
   Filtered and Trimmed FASTQs
** Learn Error Rates: Output
   error model

   | Phred | A:A | A:T | A:C | A:G | C:A | ... | G:G |
   |     / | <>  | <>  | <>  | <>  | <>  | <>  | <>  |
   |-------+-----+-----+-----+-----+-----+-----+-----|
   |     1 |     |     |     |     |     |     |     |
   |     2 |     |     |     |     |     |     |     |
   |     3 |     |     |     |     |     |     |     |
   |   ... |     |     |     |     |     |     |     |
   |    40 |     |     |     |     |     |     |     |
** Learn Error Rates: Tools
   dada2::learnErrors()
** Learn Error Rates: Notes
   Separate error models need to be built for R1 and R2
* Dereplication
** Dereplication: Why?
   Summarize reads into unique observed reads, with quality summary and count

*** A screenshot 					    :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
1. CAGCT
2. TATAA
3. TATAA
4. TGCGC
5. CGGGC
6. TGCcC
7. TGCGC
8. CAGCT
9. CGGGa
10. TGCGC
*** A text section 						      :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    | Sequence | Count | Quality |
    | /        |    <> |      <> |
    |----------+-------+---------|
    | CAGCT    |     2 |   99989 |
    |----------+-------+---------|
    | TATAA    |     2 |   99998 |
    |----------+-------+---------|
    | TGCGC    |     3 |   99988 |
    |----------+-------+---------|
    | CGGGC    |     1 |   99999 |
    |----------+-------+---------|
    | TGCcC    |     1 |   99948 |
    |----------+-------+---------|
    | CGGGa    |     1 |   99993 |
    |----------+-------+---------|

** Dereplication: Input 
   Filtered and Trimmed FASTQs
** Dereplication: Output
   Unique reads with summarized quality and counts
** Dereplication: Tools
   dada2::derepFastq()
** Dereplication: Notes
   Dereplication is done separately for R1 and R2
* Sample Inference
** Sample Inference: Why?
   Attempt to determine the true sequences from which reads were derived

*** A text section 						      :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    | Sequence | Count | Quality |
    | /        |    <> |      <> |
    |----------+-------+---------|
    | CAGCT    |     2 |   99989 |
    |----------+-------+---------|
    | TATAA    |     2 |   99998 |
    |----------+-------+---------|
    | TGCGC    |     3 |   99988 |
    |----------+-------+---------|
    | CGGGC    |     1 |   99999 |
    |----------+-------+---------|
    | TGCcC    |     1 |   99948 |
    |----------+-------+---------|
    | CGGGa    |     1 |   99993 |
    |----------+-------+---------|

*** A text section 						      :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
    | Sequence | Count |
    | /        |    <> |
    |----------+-------|
    | CAGCT    |     2 |
    |----------+-------|
    | TATAA    |     2 |
    |----------+-------|
    | TGCGC    |     4 |
    |----------+-------|
    | CGGGC    |     2 |
    |----------+-------|
** Sample Inference: Input 
   - Dereplicated Reads
   - Error Model
** Sample Inference: Output
   Inferred read sequences with counts

** Sample Inference: Tools
   dada2::dada()
** Sample Inference: Notes
   Sample Inference is done separately for R1 and R2

* Merge Paired Reads
** Merge Paired Reads: Why?
   Collapse read pairs into a single sequence for each inferred amplicon

#+begin_src text
R1:       ATACCCTAGTGC
R2:          CCCTAGTGCCGT

Merged:   ATACCCTAGTGCCGT
#+end_src 


** Merge Paired Reads: Input 
   - R1 
     - Inferred Sequences 
     - Dereplicated Sequences 
   - R2 
     - Inferred Sequences 
     - Dereplicated Sequences 
** Merge Paired Reads: Output
Inferred amplicon sequences

** Merge Paired Reads: Tools
   dada2::mergePairs()

* Construct Sequence Table
** Construct Sequence Table: Why?
Generate count table

|            | Sample 1 | Sample 2 | …  | Sample N |
| /          | <>       | <>       | <> | <>       |
|------------+----------+----------+----+----------|
| Bacteria 1 |          |          |    |          |
|------------+----------+----------+----+----------|
| Bacteria 2 |          |          |    |          |
|------------+----------+----------+----+----------|
| …          |          |          |    |          |
|------------+----------+----------+----+----------|
| Bacteria N |          |          |    |          |
|------------+----------+----------+----+----------|

** Construct Sequence Table: Input 
   Merged sequences
** Construct Sequence Table: Output
   Count table
** Construct Sequence Table: Tools
   dada2::makeSequenceTable()

* Remove Chimeras
** Remove Chimeras: Why?
   Library preparation is imperfect, so it generates chimeric amplicons
** Remove Chimeras: Input 
   Count Table
** Remove Chimeras: Output
   Count table *without* chimeras
** Remove Chimeras: Tools
   dada2::removeBimeraDenovo()

* Assign Taxonomy
** Assign Taxonomy: Why?
   Relate sequences in our count table to specific bacteria
   
** Assign Taxonomy: Input 
   Chimera-free merged sequences
** Assign Taxonomy: Output
   Mapping from sequences to specific bacteria
** Assign Taxonomy: Tools
   dada2::assignTaxonomy()



* Generate Phyloseq Object
** Generate Phyloseq Object: Why?
   Phyloseq objects organize multiple aspects of our results and ease downstream analysis and visualization
** Generate Phyloseq Object: Input 
   - Count Table
   - Metadata Table
   - Taxonomic Assignment
   - Phylogenetic Tree (optional)
** Generate Phyloseq Object: Output
   Phyloseq Object
** Generate Phyloseq Object: Tools
   phyloseq::phyloseq()

* Save Phyloseq as RDS
** Save Phyloseq as RDS: Why?
   - Generating the final phyloseq object from raw FASTQs is time consuming, we would prefer to not repeat it everytime we want to play with the results
   - The Phyloseq object is a very space efficient representation of the processed data
** Save Phyloseq as RDS: Input
   - Phyloseq object
   - Name for RDS file
** Save Phyloseq as RDS: Output
    RDS file
** Save Phyloseq as RDS: Tools
   readr::write_rds()


* Pre-DADA2 Preparation||||||   :noexport:

** Demultiplex


** Adapter Trimming
** Read Synchronization

* DADA2 Overview||||||   :noexport:
** Method Comparison
- DADA2
- Mothur
- QIIME1
- QIIME2
- Others?

** DADA2 Input
- Demultiplexed FASTQs


* Filter and Trim||||||   :noexport:

** Examine quality profiles of forward and reverse reads

** Perform filtering and trimming

Assign the filenames for the filtered fastq.gz files.
```{r filt-names}
filt_path <- file.path(scratch.dir, "filtered") # Place filtered files in filtered/ subdirectory
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq.gz"))
```

We'll use standard filtering parameters: `maxN=0` (DADA2 requires no Ns), `truncQ=2`, `rm.phix=TRUE` and `maxEE=2`. The `maxEE` parameter sets the maximum number of "expected errors" allowed in a read, which is [a better filter than simply averaging quality scores](http://www.drive5.com/usearch/manual/expected_errors.html).

**Filter the forward and reverse reads**
```{r filter, message=FALSE, warning=FALSE}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)
```

<div style="border: 1px solid red;padding: 5px;background-color: #fff6f6;">**<span style="color:red">If using this workflow on your own data:</span>** The standard filtering parameters are starting points, not set in stone. For example, if too few reads are passing the filter, considering relaxing `maxEE`, perhaps especially on the reverse reads (eg. `maxEE=c(2,5)`). If you want to speed up downstream computation, consider tightening `maxEE`. For paired-end reads consider the length of your amplicon when choosing `truncLen` as your reads must overlap after truncation in order to merge them later.</div>

<div style="border: 1px solid red;padding: 5px;background-color: #fff6f6;margin-top: 15px;">**<span style="color:red">If using this workflow on your own data:</span>** For common ITS amplicon strategies, it is undesirable to truncate reads to a fixed length due to the large amount of length variation at that locus. That is OK, just leave out `truncLen`. Make sure you removed the forward and reverse primers from both the forward and reverse reads though!</div>

&nbsp;

* Learn the Error Rates||||||   :noexport:

The DADA2 algorithm depends on a parametric error model (`err`) and every amplicon dataset has a different set of error rates. The `learnErrors` method learns the error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many optimization problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

*The following runs in about 4 minutes on a 2013 Macbook Pro:*

```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
```

It is always worthwhile, as a sanity check if nothing else, to visualize the estimated error rates:
```{r plot-errors, warning=FALSE}
plotErrors(errF, nominalQ=TRUE)
```

The error rates for each possible transition (eg. A->C, A->G, ...) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence. The red line shows the error rates expected under the nominal definition of the Q-value. Here the black line (the estimated rates) fits the observed rates well, and the error rates drop with increased quality as expected. Everything looks reasonable and we proceed with confidence.

<div style="border: 1px solid red;padding: 5px;background-color: #fff6f6;">**<span style="color:red">If using this workflow on your own data:</span>** Parameter learning is computationally intensive, so by default the `learnErrors` function uses only a subset of the data (the first 1M reads). If the plotted error model does not look like a good fit, try increasing the `nreads` parameter to see if the fit improves.</div>

&nbsp;

* Dereplication|||||||   :noexport:

Dereplication combines all identical sequencing reads into into "unique sequences" with a corresponding "abundance": the number of reads with that unique sequence. Dereplication substantially reduces computation time by eliminating redundant comparisons.

Dereplication in the DADA2 pipeline has one crucial addition from other pipelines: **DADA2 retains a summary of the quality information associated with each unique sequence**. The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads. These quality profiles inform the error model of the subsequent denoising step, significantly increasing DADA2's accuracy.

**Dereplicate the filtered fastq files**
```{r dereplicate, message=FALSE}
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
* Name the derep-class objects by the sample names||   :noexport:
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

<div style="border: 1px solid red;padding: 5px;background-color: #fff6f6;">**<span style="color:red">If using this workflow on your own data:</span>** The tutorial dataset is small enough to easily load into memory. If your dataset exceeds available RAM, it is preferable to process samples one-by-one in a streaming fashion: see the [DADA2 Workflow on Big Data](bigdata.html) for an example.</div>

&nbsp;

* Sample Inference||||||   :noexport:

We are now ready to apply the core sequence-variant inference algorithm to the dereplicated data. 

**Infer the sequence variants in each sample**
```{r dada}
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
```

Inspecting the dada-class object returned by dada:
```{r see-dada}
dadaFs[[1]]
```


<div style="border: 1px solid red;padding: 5px;background-color: #fff6f6;">**<span style="color:red">If using this workflow on your own data:</span>** All samples are simultaneously loaded into memory in the tutorial. If you are dealing with datasets that approach or exceed available RAM, it is preferable to process samples one-by-one in a streaming fashion: see the **[DADA2 Workflow on Big Data](bigdata.html)** for an example.</div>

<div style="border: 1px solid red;padding: 5px;background-color: #fff6f6;margin-top: 15px;">**<span style="color:red">If using this workflow on your own data:</span>** By default, the `dada` function processes each sample independently, but pooled processing is available with `pool=TRUE` and that may give better results for low sampling depths at the cost of increased computation time. See our [discussion about pooling samples for sample inference](pool.html).</div>

<div style="border: 1px solid red;padding: 5px;background-color: #fff6f6;margin-top: 15px;">**<span style="color:red">If using this workflow on your own data:</span>** DADA2 also supports 454 and Ion Torrent data, but [we recommend some minor parameter changes](faq.html#can-i-use-dada2-with-my-454-or-ion-torrent-data) for those sequencing technologies. The adventurous can explore `?setDadaOpt` for other adjustable algorithm parameters.</div>

&nbsp;

* Merge paired reads||||||   :noexport:

Spurious sequence variants are further reduced by merging overlapping reads. The core function here is `mergePairs`, which depends on the forward and reverse reads being in matching order at the time they were dereplicated.

**Merge the denoised forward and reverse reads**:
```{r merge, message=FALSE}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
* Inspect the merger data.frame from the first sample||   :noexport:
head(mergers[[1]])
```

We now have a `data.frame` for each sample with the merged `$sequence`, its `$abundance`, and the indices of the merged `$forward` and `$reverse` denoised sequences. Paired reads that did not exactly overlap were removed by `mergePairs`.

<div style="border: 1px solid red;padding: 5px;background-color: #fff6f6;">**<span style="color:red">If using this workflow on your own data:</span>** Most of your **reads** should successfully merge. If that is not the case upstream parameters may need to be revisited: Did you trim away the overlap between your reads?</div>

&nbsp;

* Construct sequence table|||||   :noexport:

We can now construct a sequence table of our mouse samples, a higher-resolution version of the OTU table produced by traditional methods.
```{r seqtab}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
* Inspect distribution of sequence lengths|||   :noexport:
table(nchar(getSequences(seqtab)))
```

The sequence table is a `matrix` with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. The lengths of our merged sequences all fall within the expected range for this V4 amplicon.

<div style="border: 1px solid red;padding: 5px;background-color: #fff6f6;">**<span style="color:red">If using this workflow on your own data:</span>** Sequences that are much longer or shorter than expected may be the result of non-specific priming, and may be worth removing (eg. `seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(250,256)]`). This is analogous to "cutting a band" in-silico to get amplicons of the targeted length.</div>

&nbsp;

* Remove chimeras||||||   :noexport:

The core `dada` method removes substitution and indel errors, but chimeras remain. Fortunately, the accuracy of the sequences after denoising makes identifying chimeras simpler than it is when dealing with fuzzy OTUs: all sequences which can be exactly reconstructed as a bimera (two-parent chimera) from more abundant sequences.

**Remove chimeric sequences**:
```{r chimeras, message=FALSE}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
```


<div style="border: 1px solid red;padding: 5px;background-color: #fff6f6;">**<span style="color:red">If using this workflow on your own data:</span>** Most of your **reads** should remain after chimera removal (it is not uncommon for a majority of **sequence variants** to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to beginning the DADA2 pipeline.</div>

&nbsp;

* Track reads through the pipeline||||   :noexport:

As a final check of our progress, we'll look at the number of reads that made it through each step in the pipeline:
```{r track}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names
head(track)
```

Looks good, we kept the majority of our raw reads, and there is no over-large drop associated with any single step.

<div style="border: 1px solid red;padding: 5px;background-color: #fff6f6;">**<span style="color:red">If using this workflow on your own data:</span>** This is a great place to do a last **sanity check**. Outside of filtering (depending on how stringent you want to be) there should no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the `truncLen` parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads failed to pass the chimera check, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification.</div>

&nbsp;

* Assign taxonomy||||||   :noexport:

It is common at this point, especially in 16S/18S/ITS amplicon sequencing, to classify sequence variants taxonomically. The DADA2 package provides a native implementation of [the RDP's naive Bayesian classifier](http://www.ncbi.nlm.nih.gov/pubmed/17586664) for this purpose. The `assignTaxonomy` function takes a set of sequences and a training set of taxonomically classified sequences, and outputs the taxonomic assignments with at least `minBoot` bootstrap confidence. 

We maintain [formatted training fastas for the RDP training set, GreenGenes clustered at 97\% identity, and the Silva reference database](training.html). For fungal taxonomy, the General Fasta release files from the [UNITE ITS database](https://unite.ut.ee/repository.php) can be used as is. To follow along, download the `silva_nr_v128_train_set.fa.gz` file, and place it in the directory with the fastq files.

```{r taxify}
taxa <- assignTaxonomy(seqtab.nochim, silva.ref, multithread=TRUE)
```

**Optional:** The dada2 package also implements a method to make [species level assignments based on **exact matching**](assign.html#species-assignment) between ASVs and sequenced reference strains. Currently species-assignment training fastas are available for the Silva and RDP 16S databases. To follow the optional species addition step, download the `silva_species_assignment_v128.fa.gz` file, and place it in the directory with the fastq files.

```{r species}
taxa <- addSpecies(taxa, silva.species.ref)
```

Let's inspect the taxonomic assignments:
```{r see-tax}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

Unsurprisingly, the Bacteroidetes are well represented among the most abundant taxa in these fecal samples. Few species assignments were made, both because it is often not possible to make unambiguous species assignments from segments of the 16S gene, and because there is surprisingly little coverage of the indigenous mouse gut microbiota in reference databases.

<div style="border: 1px solid red;padding: 5px;background-color: #fff6f6;">**<span style="color:red">If using this workflow on your own data:</span>** If your reads do not seem to be appropriately assigned, for example lots of your bacterial 16S sequences are being assigned as `Eukaryota NA NA NA NA NA`, your reads may be in the opposite orientation as the reference database. Tell dada2 to try the reverse-complement orientation with `assignTaxonomy(..., tryRC=TRUE)` and see if this fixes the assignments.</div>

&nbsp;

* Evaluate accuracy||||||   :noexport:

One of the samples included here was a "mock community", in which a mixture of 20 known strains was sequenced (this mock community is supposed to be 21 strains, but *P. acnes* is absent). Reference sequences corresponding to these strains were provided in the downloaded zip archive. We return to that sample and compare the sequence variants inferred by DADA2 to the expected composition of the community.

**Evaluating DADA2's accuracy on the mock community**:
```{r accuracy}
unqs.mock <- seqtab.nochim["Mock",]
unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community.\n")

mock.ref <- getSequences(file.path(miseqsop.dir, "HMP_MOCK.v35.fasta"))
match.ref <- sum(sapply(names(unqs.mock), function(x) any(grepl(x, mock.ref))))
cat("Of those,", sum(match.ref), "were exact matches to the expected reference sequences.\n")
```

This mock community contained **20** bacterial strains. DADA2 identified **20** ASVs all of which **exactly** match the reference genomes of the expected community members. The residual error rate after the DADA2 pipeline for this sample is **0\%**.

In comparison, [the mothur pipeline finds 34 OTUs in this Mock community sample](http://www.mothur.org/wiki/MiSeq_SOP#Assessing_error_rates). DADA2 infers sequence variants exactly instead of fuzzy 97\% OTUs, and outputs fewer false positives to boot!

**Here ends the DADA2 portion of the tutorial**.

---------------------------------------------------------

* Bonus: Handoff to phyloseq|||||   :noexport:

The [phyloseq R package is a powerful framework for further analysis of microbiome data](https://joey711.github.io/phyloseq/). We now demosntrate how to straightforwardly import the tables produced by the DADA2 pipeline into phyloseq. We'll also add the small amount of metadata we have -- the samples are named by the gender (G), mouse subject number (X) and the day post-weaning (Y) it was sampled (eg. GXDY).

**Import into phyloseq**:
```{r phyloseq, message=FALSE, warning=FALSE}
library(phyloseq); packageVersion("phyloseq")
library(ggplot2); packageVersion("ggplot2")
```

We can construct a simple sample data.frame based on the filenames. Usually this step would instead involve reading the sample data in from a file.
```{r make-sample-data}
samples.out <- rownames(seqtab.nochim)
subject <- sapply(strsplit(samples.out, "D"), `[`, 1)
gender <- substr(subject,1,1)
subject <- substr(subject,2,999)
day <- as.integer(sapply(strsplit(samples.out, "D"), `[`, 2))
samdf <- data.frame(Subject=subject, Gender=gender, Day=day)
samdf$When <- "Early"
samdf$When[samdf$Day>100] <- "Late"
rownames(samdf) <- samples.out
```

We can now construct a phyloseq object directly from the dada2 outputs.
```{r make-phyloseq}
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))
ps <- prune_samples(sample_names(ps) != "Mock", ps) # Remove mock sample
ps
```

Any R object can be saved to an RDS file.  It is a good idea to do this for any object that is time consuming to generate and is reasonably small in size.  Even when the object was generated reproducibly, it can be frustrating to wait minutes or hours to regenerate when you are ready to perform downstream analyses.

We will do this for out phyloseq object to a file since it is quite small (especially compared to the size of the input FASTQ files), and there were several time consuming computational steps required to generate it.  
```{r}
write_rds(ps, ps.rds)
```

We can now confirm that it worked!
```{r}
ps = read_rds(ps.rds)
```


We are now ready to use phyloseq!

**Visualize alpha-diversity**:
```{r richness, warning=FALSE}
plot_richness(ps, x="Day", measures=c("Shannon", "Simpson"), color="When") + theme_bw()
```

No obvious systematic difference in alpha-diversity between early and late samples.

**Ordinate**:
```{r ordinate}
ord.nmds.bray <- ordinate(ps, method="NMDS", distance="bray")
plot_ordination(ps, ord.nmds.bray, color="When", title="Bray NMDS")
```

Ordination picks out a clear separation between the early and late samples.

**Bar plot**:
```{r bar-plot}
top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
plot_bar(ps.top20, x="Day", fill="Family") + facet_wrap(~When, scales="free_x")
```

Nothing glaringly obvious jumps out from the taxonomic distribution of the top 20 sequences to explain the early-late differentiation.

This was just a bare bones demonstration of how the data from DADA2 can be easily imported into phyloseq and interrogated. For further examples on the many analyses possible with phyloseq, see [the phyloseq web site](https://joey711.github.io/phyloseq/)!

* Beamer configuration ||||||   :noexport:
  :CONFIGURATION: 
#+DESCRIPTION: 
#+KEYWORDS: 
#+LANGUAGE:  en
#+OPTIONS:   num:t toc:t ::t |:t ^:{} -:t f:t *:t <:t
#+OPTIONS:   tex:t d:nil todo:nil pri:nil tags:nil
#+OPTIONS:   timestamp:t

# this allows defining headlines to be exported/not be exported
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

** Basic
# this triggers loading the beamer menu (C-c C-b) when the file is read
#+startup: beamer

#+LaTeX_CLASS: beamer

#    LATEX CLASS OPTIONS
# [bigger]
# [presentation]
# [handout] : print handouts, i.e. slides with overlays will be printed with
#   all overlays turned on (no animations).
# [notes=show] : show notes in the generated output (note pages follow the real page)
# [notes=only] : only render the nodes pages

# this setting affects whether the initial PSI picture correctly fills
# the title page, since it scales the title text. One can also use the
# notes=show or notes=only options to produce notes pages in the output.
# #+LaTeX_CLASS_OPTIONS: [t,10pt,notes=show]

#+LaTeX_CLASS_OPTIONS: [t,12pt]


#+COLUMNS: %20ITEM %13BEAMER_env(Env) %6BEAMER_envargs(Args) %4BEAMER_col(Col) %7BEAMER_extra(Extra)

# export second level headings as beamer frames. All headlines below
# the org-beamer-frame-level (i.e. below H value in OPTIONS), are
# exported as blocks
#+OPTIONS: H:2

** Beamer Theme Definition
# https://hartwork.org/beamer-theme-matrix/
# http://www.deic.uab.es/%7Eiblanes/beamer_gallery/index_by_theme_and_color.html 

# ------------------------
#+BEAMER_THEME: default 
#+BEAMER_COLOR_THEME: rose 
# ------------------------

# Note: custom style files can be placed centrally in the user specific directory
# ~/texmf/tex. This will be searched recursively, so substructures are possible.
# q.v. http://tex.stackexchange.com/questions/1137/where-do-i-place-my-own-sty-or-cls-files-to-make-them-available-to-all-my-te

# One could also fine tune a number of theme settings instead of specifying the full theme
# #+BEAMER_COLOR_THEME: default
# #+BEAMER_FONT_THEME:
# #+BEAMER_INNER_THEME:
# #+BEAMER_OUTER_THEME: miniframes [subsection=false]
# #+LATEX_CLASS: beamer


# Get rid of navigation bullets at top?
#+BEAMER_HEADER: \beamertemplatenavigationsymbolsempty


** changes to BeginSection for TOC and navigation
#+BEAMER_HEADER: \AtBeginSection[]{

# This line inserts a table of contents with the current section highlighted at
# the beginning of each section
#+BEAMER_HEADER: \begin{frame}<beamer>\frametitle{Topic}\tableofcontents[currentsection]\end{frame}

# In order to have the miniframes/smoothbars navigation bullets even though we do not use subsections 
# q.v. https://tex.stackexchange.com/questions/2072/beamer-navigation-circles-without-subsections/2078#2078
#+BEAMER_HEADER: \subsection{}
#+BEAMER_HEADER: }

** misc configuration
# I want to define a style for hyperlinks
#+BEAMER_HEADER: \hypersetup{colorlinks=true, linkcolor=blue}

# this can be used to define the transparency of the covered layers
#+BEAMER: \setbeamercovered{transparent=30}
# #+LaTeX_HEADER: \def\thefootnote{\xdef\@thefnmark{}\@footnotetext}

# Set footnote mark to "white" so it is not visible (i.e. blends in with background)
# #+BEAMER_HEADER: \setbeamercolor{footnote mark}{fg=white}

# Get rid of navigation symbols in bottom right of slide
#+BEAMER_HEADER: \beamertemplatenavigationsymbolsempty

# Trying to move footnotes
# #+BEAMER_HEADER: \setbeamertemplate{footnote}{%
# #+BEAMER_HEADER:   \parindent 0em\noindent%
# #+BEAMER_HEADER:   \raggedright
# #+BEAMER_HEADER:   \usebeamercolor{footnote}\hbox to 0.8em{\hfil\insertfootnotemark}\insertfootnotetext\par%
# #+BEAMER_HEADER: }

# Smaller footnotes
#+BEAMER_HEADER: \setbeamerfont{footnote}{size=\tiny}

** Some remarks on options
   - [[info:org#Export%20settings][info:org#Export settings]]
   - The H:2 setting in the options line is important for setting the
     Beamer frame level. Headlines will become frames when their level
     is equal to =org-beamer-frame-level=.
   - ^:{} interpret abc_{subs} as subscript, but not abc_subs
   - num:t configures whether to use section numbers. If set to a number
     only headlines of this level or above will be numbered
   - ::t defines that lines starting with ":" will use fixed width font
   - |:t include tables in export
   - -:t Non-nil means interpret "\-", "--" and "---" for export.
   - f:t include footnotes
   - *:t Non-nil means interpret
     : *word*, /word/, _word_ and +word+.
   - <:t toggle inclusion of timestamps
   - timestamp:t include a document creation timestamp into the exported file
   - todo:t include exporting of todo keywords
   - d:nil do not export org heading drawers
   - tags:nil do not export headline tags

** addtional LaTeX packages

   # for generating example texts for testing
   #+BEAMER_HEADER: \usepackage{blindtext}
:END:

